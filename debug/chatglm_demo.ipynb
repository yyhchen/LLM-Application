{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f762543-ff32-4ebe-952d-53431749fb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21b9877bbc7405ab5b442edd0a6428f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\anaconda3\\envs\\chatGLM_env\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ðŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿Žé—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n",
      "\n",
      "ç¡çœ å¯¹èº«ä½“å¥åº·å’Œå¿ƒç†å¥åº·éƒ½å¾ˆé‡è¦ï¼Œå¦‚æžœä½ æ™šä¸Šç¡ä¸ç€ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹ä¸€äº›æ–¹æ³•æ¥å¸®åŠ©è‡ªå·±å…¥ç¡:\n",
      "\n",
      "1. æ”¹å–„ç¡çœ çŽ¯å¢ƒ:ç¡®ä¿ç¡çœ çŽ¯å¢ƒå®‰é™ã€èˆ’é€‚ã€é»‘æš—ã€å‡‰çˆ½ã€‚å¦‚æžœç¡çœ çŽ¯å¢ƒä¸åˆé€‚ï¼Œä¼šæ„Ÿåˆ°ç–²åŠ³å’Œä¸å®‰ï¼Œä»Žè€Œæ›´éš¾å…¥ç¡ã€‚\n",
      "\n",
      "2. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡è§‰å‰ä¸€å°æ—¶ï¼Œæ”¾æ¾èº«å¿ƒæ˜¯å¾ˆé‡è¦çš„ã€‚ä½ å¯ä»¥åšäº›è½»æ¾çš„ä¼¸å±•è¿åŠ¨ã€å†¥æƒ³æˆ–è¿›è¡Œä¼¸å±•è¿åŠ¨ï¼Œæˆ–åšç‘œä¼½ã€‚\n",
      "\n",
      "3. è§„å¾‹ä½œæ¯:å°½é‡åœ¨åŒä¸€æ—¶é—´å…¥ç¡å’Œèµ·åºŠï¼Œå³ä½¿åœ¨å‘¨æœ«ä¹Ÿåº”è¯¥ä¿æŒç›¸åŒçš„ä½œæ¯æ—¶é—´ï¼Œè¿™æœ‰åŠ©äºŽè°ƒèŠ‚èº«ä½“çš„ç”Ÿç‰©é’Ÿã€‚\n",
      "\n",
      "4. é¿å…åˆºæ¿€:ç¡è§‰å‰ä¸€å°æ—¶ï¼Œå°½é‡é¿å…çœ‹ç”µè§†ã€çŽ©æ¸¸æˆæˆ–è¿›è¡Œåˆºæ¿€æ€§çš„æ´»åŠ¨ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šè®©ä½ å…´å¥‹å’Œä¸å®‰ã€‚\n",
      "\n",
      "5. é¿å…å’–å•¡å› å’Œé…’ç²¾:å’–å•¡å› å’Œé…’ç²¾éƒ½å¯èƒ½å½±å“ç¡çœ è´¨é‡ï¼Œå› æ­¤åœ¨ç¡è§‰å‰ä¸€å°æ—¶ï¼Œå°½é‡é¿å…æ‘„å…¥è¿™äº›ç‰©è´¨ã€‚\n",
      "\n",
      "6. è¿œç¦»åŽ‹åŠ›:åœ¨ç¡è§‰å‰ä¸€å°æ—¶ï¼Œå°½é‡é¿å…è¿›è¡Œç´§å¼ çš„æ´»åŠ¨ï¼Œå¦‚æ¿€çƒˆçš„è¿åŠ¨æˆ–ç´§å¼ çš„å·¥ä½œã€‚\n",
      "\n",
      "å¦‚æžœè¿™äº›æ–¹æ³•ä¸èƒ½è§£å†³ä½ çš„é—®é¢˜ï¼Œå¯ä»¥å°è¯•å¯»æ±‚åŒ»ç”Ÿçš„å¸®åŠ©ï¼Œæ‰¾åˆ°æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    æºè‡ªæ¸…åŽå¤§å­¦ github ç½‘ç«™ä¸Šçš„æ¡ˆä¾‹ï¼ˆhttps://github.com/THUDM/ChatGLM-6Bï¼‰\n",
    "\n",
    "    è°ƒè¯•é‡åŒ– int4çº§åˆ«\n",
    "\n",
    "    å…ˆä¿è¯æœ¬åœ°å®‰è£…è°ƒè¯•å®ŒCUDAï¼Œç„¶åŽåˆ›å»ºconda çŽ¯å¢ƒ è¿›è¡ŒçŽ¯å¢ƒé…ç½® pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_path = \"D:\\CodeLibrary\\ChatGLM\\chatglm26b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device='cuda')\n",
    "# æŒ‰éœ€ä¿®æ”¹ï¼Œç›®å‰åªæ”¯æŒ 4/8 bit é‡åŒ–\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).quantize(4).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    "print(response + '\\n')\n",
    "\n",
    "response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€Žä¹ˆåŠž\", history=history)\n",
    "print(response + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfbe3d-3413-497f-91e2-e22fe2cdedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"è¿˜æœ‰å…¶ä»–çš„å—\", history=history)\n",
    "print(response + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8c1fb-45b4-49f1-ad80-1c5e9378f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# èŽ·å–å½“å‰å·¥ä½œç›®å½•\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "print(\"å½“å‰å·¥ä½œç›®å½•:\", current_working_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4fb98d-9204-407d-aeff-080aea2230cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd01587-30bc-489a-9de9-61aa0fa81e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æœ‰å“ªäº›ç±»\n",
    "from langchain import embeddings\n",
    "import inspect\n",
    "classes = [m[0] for m in inspect.getmembers(embeddings, inspect.isclass)]\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e87fd-f28c-4317-9387-bfc7931517a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹åŒ…é‡Œé¢å…·ä½“çš„ç±» ä»£ç \n",
    "print(inspect.getsource(embeddings.HuggingFaceHubEmbeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ee91a-c1ca-4ddf-9277-831d35fe3695",
   "metadata": {},
   "source": [
    "# æ ¹æ®langchain å’Œ Faissæž„å»ºçŸ¥è¯†å‘é‡åº“Â Â Â Â Â Â Â \n",
    "\n",
    "å…¶ä¸­ä½¿ç”¨ `pip install faiss-gpu` å®‰è£… GPUç‰ˆæœ¬çš„ Faisså‘é‡æ•°æ®åº“\n",
    "\n",
    "\n",
    "\n",
    "é¦–å…ˆå¼•å…¥ç›¸å…³çš„åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520324b-ef16-44ef-b181-82f01af9fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISSÂ Â Â Â \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ae747-093f-4295-8b9a-718ea61a9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb32914-8488-47d5-b87a-15e2a943d2c3",
   "metadata": {},
   "source": [
    "# å–æ¶ˆç”¨ Faissï¼Œwindowä¸æ”¯æŒï¼Œæˆ–è€…è¯´è¦æ±‚ç‰¹åˆ«éº»çƒ¦ï¼Œå¹¶ä¸”æ²¡è®²è®²æ¸…æ¥šå›ºå®šçš„ä¾èµ–ï¼Œå¯¼è‡´æ— æ³•å®‰è£…\n",
    "\n",
    "\n",
    "\n",
    "ä¸»è¦ä½“çŽ°åœ¨cudaç‰ˆæœ¬ï¼Œè¿˜æœ‰æ”¯ä¸æ”¯æŒgpuçš„é—®é¢˜ï¼›è¦åœ¨linuxä¸Šæ¯”è¾ƒå®¹æ˜“ï¼Œæ•…æ”¾å¼ƒï¼ˆæµªè´¹æ—¶é—´é…çŽ¯å¢ƒï¼‰Â Â Â \n",
    "\n",
    "# æ¢æˆmilvusï½žÂ Â Â Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc4b8d-652f-4afe-9306-3e263dbda1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# import milvus\n",
    "# from langchain.vectorstores import milvus\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c6339-37c3-4c93-9875-9d20aa7d0d38",
   "metadata": {},
   "source": [
    "# åŠ è½½æ•°æ®ï¼Œæ ¼å¼ä¸ºmdæ–‡ä»¶Â Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e890981c-335a-4ab1-a5d3-e159e1793661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data example\n",
      "`Beam search` æ˜¯ä¸€ç§å¯å‘å¼æœç´¢ç®—æ³•ï¼Œå¸¸ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è§£ç é˜¶æ®µï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ä¸­ã€‚å®ƒçš„ä¸»è¦ç›®çš„æ˜¯åœ¨è§£ç è¿‡ç¨‹ä¸­ä¿æŒä¸€ä¸ªå€™é€‰åˆ—è¡¨ï¼ˆå³â€œbeamâ€ï¼‰ï¼Œè¿™ä¸ªåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªéƒ¨åˆ†æž„å»ºçš„åºåˆ—ï¼ˆå¦‚å¥å­çš„ä¸€éƒ¨åˆ†ï¼‰ï¼Œå¹¶ä¸”æ ¹æ®æŸç§è¯„åˆ†æœºåˆ¶ï¼ˆå¦‚æ¦‚çŽ‡ï¼‰å¯¹è¿™äº›å€™é€‰è¿›è¡ŒæŽ’åºå’Œç­›é€‰ã€‚\n",
      "\n",
      "`Beam search` çš„**æ ¸å¿ƒæ€æƒ³**æ˜¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼ˆæˆ–æ¯ä¸ªè¯æ±‡çš„ç”Ÿæˆæ­¥éª¤ï¼‰ä¸ä»…ä»…è€ƒè™‘å•ä¸€çš„æœ€ä¼˜å€™é€‰ï¼Œè€Œæ˜¯ç»´æŠ¤ä¸€ä¸ªå¤§å°ä¸º B çš„å€™é€‰é›†ï¼ˆB é€šå¸¸è¢«ç§°ä¸º beam width æˆ– beam sizeï¼‰ï¼Œè¿™æ ·å¯ä»¥åœ¨è§£ç è¿‡ç¨‹ä¸­æŽ¢ç´¢å¤šæ¡è·¯å¾„ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œç®—æ³•éƒ½ä¼šæ ¹æ®å½“å‰çš„å€™é€‰é›†æ‰©å±•å‡ºæ–°çš„å€™é€‰é›†ï¼Œå¹¶ä»Žä¸­é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ B ä¸ªå€™é€‰ï¼Œä½œä¸ºä¸‹ä¸€æ­¥çš„å€™é€‰é›†ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸€ç›´æŒç»­åˆ°ç”Ÿæˆåºåˆ—çš„ç»“æŸç¬¦è¢«æ·»åŠ åˆ°æŸä¸ªå€™é€‰ä¸­ï¼Œæˆ–è€…è¾¾åˆ°åºåˆ—çš„æœ€å¤§é•¿åº¦é™åˆ¶ã€‚\n",
      "\n",
      "\n",
      "ä¸‹é¢é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜Ž beam search çš„è¿‡ç¨‹ï¼š\n",
      "å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¯­è¨€æ¨¡åž‹ï¼Œå®ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šä¸ºä¸‹ä¸€ä¸ªè¯æ±‡ç”Ÿæˆä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒã€‚æˆ‘ä»¬è¦ä½¿ç”¨è¿™ä¸ªæ¨¡åž‹æ¥ç”Ÿæˆä¸€ä¸ªå¥å­ï¼Œå¹¶ä¸”è®¾ç½® beam size ä¸º 3ã€‚\n",
      "1. ç¬¬ä¸€æ­¥ï¼Œæ¨¡åž‹ç”Ÿæˆäº†ä¸‰ä¸ªè¯æ±‡çš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œæˆ‘ä»¬é€‰æ‹©æ¦‚çŽ‡æœ€é«˜çš„ä¸‰ä¸ªè¯æ±‡ä½œä¸ºå½“å‰æ­¥çš„å€™é€‰ï¼š[\"æˆ‘\"ï¼Œ\"ä½ \"ï¼Œ\"ä»–\"]ã€‚\n",
      "2. ç¬¬äºŒæ­¥ï¼Œå¯¹äºŽæ¯ä¸ªå€™é€‰ï¼Œæ¨¡åž‹ç”ŸæˆæŽ¥ä¸‹æ¥ä¸€ä¸ªè¯æ±‡çš„æ¦‚çŽ‡åˆ†å¸ƒã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªå€™é€‰è¯æ±‡ç»„åˆè®¡ç®—ä¸€ä¸ªè”åˆæ¦‚çŽ‡ï¼Œå¹¶é€‰æ‹©æ¦‚çŽ‡æœ€é«˜çš„ä¸‰ä¸ªç»„åˆä½œä¸ºæ–°çš„å€™é€‰é›†ã€‚ä¾‹å¦‚ï¼Œå¯èƒ½çš„æ–°å€™é€‰é›†æ˜¯ï¼š[\"æˆ‘ çˆ±\"ï¼Œ\"ä½  æ˜¯\"ï¼Œ\"ä»– çš„\"]ã€‚\n",
      "3. é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æŸä¸ªå€™é€‰åºåˆ—ä»¥å¥å­ç»“æŸç¬¦ï¼ˆå¦‚å¥å·ï¼‰ç»“æŸï¼Œæˆ–è€…è¾¾åˆ°åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚\n",
      "\n",
      "\n",
      "`Beam search` å…è®¸ç®—æ³•åœ¨è§£ç è¿‡ç¨‹ä¸­è€ƒè™‘å¤šä¸ªå¯èƒ½çš„å€™é€‰ï¼Œä»Žè€Œ**åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¹³è¡¡äº†è´ªå¿ƒç®—æ³•ï¼ˆæ¯æ¬¡é€‰æ‹©æœ€ä¼˜å€™é€‰ï¼‰å’Œç©·ä¸¾æœç´¢ï¼ˆè€ƒè™‘æ‰€æœ‰å¯èƒ½çš„å€™é€‰ï¼‰ä¹‹é—´çš„æƒè¡¡**ã€‚é€šè¿‡è°ƒæ•´ beam sizeï¼Œæˆ‘ä»¬å¯ä»¥æŽ§åˆ¶ç®—æ³•åœ¨è§£ç è¿‡ç¨‹ä¸­æŽ¢ç´¢çš„å¹¿åº¦å’Œæ·±åº¦ã€‚è¾ƒå¤§çš„ beam size å¯ä»¥æé«˜æ‰¾åˆ°æœ€ä¼˜è§£çš„å¯èƒ½æ€§ï¼Œä½†åŒæ—¶ä¹Ÿä¼šå¢žåŠ è®¡ç®—æˆæœ¬ã€‚\n",
      "éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒBeam search å¹¶ä¸ä¿è¯èƒ½æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¼šåœ¨è§£ç è¿‡ç¨‹ä¸­ä¸¢å¼ƒæœ€ç»ˆçš„æœ€ä¼˜è·¯å¾„ã€‚æ­¤å¤–ï¼ŒBeam search ä¹Ÿä¸ä¿è¯èƒ½å¾—åˆ°å¤šæ ·æ€§å¾ˆå¼ºçš„è¾“å‡ºï¼Œå› ä¸ºå®ƒå€¾å‘äºŽé€‰æ‹©æ¦‚çŽ‡è¾ƒé«˜çš„å€™é€‰ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„å¥å­æ¯”è¾ƒå•ä¸€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ‰æ—¶ä¼šé‡‡ç”¨ä¸€äº›é¢å¤–çš„æŠ€æœ¯ï¼Œå¦‚éšæœºåŒ–æˆ–é•¿åº¦æƒ©ç½šï¼Œæ¥å¢žåŠ è¾“å‡ºå¥å­çš„å¤šæ ·æ€§ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ps = list(Path('D:/Notes/NLP review').glob(\"**/*.md\"))\n",
    "\n",
    "data = []\n",
    "sources = []\n",
    "for p in ps:\n",
    "    with open(p, encoding='utf8') as f:\n",
    "        data.append(f.read())\n",
    "    sources.append(p)\n",
    "print(\"data example\")\n",
    "print(data[0])\n",
    "# print(\"-\"*50)\n",
    "# print(sources[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3472e-6532-4912-a366-eb2c96dea78b",
   "metadata": {},
   "source": [
    "# ç”±äºŽ LLMæœ‰contexté™åˆ¶ï¼Œæ•…éœ€è¦spliterï¼Œ`chunk_size`å…ˆéšæ„è®¾ç½®Â Â Â \n",
    "\n",
    "- `metadatas` çš„ä½œç”¨ï¼š**æ¯ä¸ªåˆ†å‰²åŽçš„æ–‡æœ¬å—ä»ç„¶éœ€è¦ä¿ç•™ä¸ŽåŽŸå§‹æ–‡æœ¬çš„å…³è”ä¿¡æ¯**Â Â \n",
    "- `splits` æ˜¯å°† text åˆ†å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21eff2f5-7dea-4c8e-819c-61ce288498df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Beam search` æ˜¯ä¸€ç§å¯å‘å¼æœç´¢ç®—æ³•ï¼Œå¸¸ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è§£ç é˜¶æ®µï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ä¸­ã€‚å®ƒçš„ä¸»è¦ç›®çš„æ˜¯åœ¨è§£ç è¿‡ç¨‹ä¸­ä¿æŒä¸€ä¸ªå€™é€‰åˆ—è¡¨ï¼ˆå³â€œbeamâ€ï¼‰ï¼Œè¿™ä¸ªåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªéƒ¨åˆ†æž„å»ºçš„åºåˆ—ï¼ˆå¦‚å¥å­çš„ä¸€éƒ¨åˆ†ï¼‰ï¼Œå¹¶ä¸”æ ¹æ®æŸç§è¯„åˆ†æœºåˆ¶ï¼ˆå¦‚æ¦‚çŽ‡ï¼‰å¯¹è¿™äº›å€™é€‰è¿›è¡ŒæŽ’åºå’Œç­›é€‰ã€‚\n",
      "`Beam search` çš„**æ ¸å¿ƒæ€æƒ³**æ˜¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼ˆæˆ–æ¯ä¸ªè¯æ±‡çš„ç”Ÿæˆæ­¥éª¤ï¼‰ä¸ä»…ä»…è€ƒè™‘å•ä¸€çš„æœ€ä¼˜å€™é€‰ï¼Œè€Œæ˜¯ç»´æŠ¤ä¸€ä¸ªå¤§å°ä¸º B çš„å€™é€‰é›†ï¼ˆB é€šå¸¸è¢«ç§°ä¸º beam width æˆ– beam sizeï¼‰ï¼Œè¿™æ ·å¯ä»¥åœ¨è§£ç è¿‡ç¨‹ä¸­æŽ¢ç´¢å¤šæ¡è·¯å¾„ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œç®—æ³•éƒ½ä¼šæ ¹æ®å½“å‰çš„å€™é€‰é›†æ‰©å±•å‡ºæ–°çš„å€™é€‰é›†ï¼Œå¹¶ä»Žä¸­é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ B ä¸ªå€™é€‰ï¼Œä½œä¸ºä¸‹ä¸€æ­¥çš„å€™é€‰é›†ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸€ç›´æŒç»­åˆ°ç”Ÿæˆåºåˆ—çš„ç»“æŸç¬¦è¢«æ·»åŠ åˆ°æŸä¸ªå€™é€‰ä¸­ï¼Œæˆ–è€…è¾¾åˆ°åºåˆ—çš„æœ€å¤§é•¿åº¦é™åˆ¶ã€‚\n",
      "ä¸‹é¢é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜Ž beam search çš„è¿‡ç¨‹ï¼š\n",
      "å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¯­è¨€æ¨¡åž‹ï¼Œå®ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šä¸ºä¸‹ä¸€ä¸ªè¯æ±‡ç”Ÿæˆä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒã€‚æˆ‘ä»¬è¦ä½¿ç”¨è¿™ä¸ªæ¨¡åž‹æ¥ç”Ÿæˆä¸€ä¸ªå¥å­ï¼Œå¹¶ä¸”è®¾ç½® beam size ä¸º 3ã€‚\n",
      "1. ç¬¬ä¸€æ­¥ï¼Œæ¨¡åž‹ç”Ÿæˆäº†ä¸‰ä¸ªè¯æ±‡çš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œæˆ‘ä»¬é€‰æ‹©æ¦‚çŽ‡æœ€é«˜çš„ä¸‰ä¸ªè¯æ±‡ä½œä¸ºå½“å‰æ­¥çš„å€™é€‰ï¼š[\"æˆ‘\"ï¼Œ\"ä½ \"ï¼Œ\"ä»–\"]ã€‚\n",
      "2. ç¬¬äºŒæ­¥ï¼Œå¯¹äºŽæ¯ä¸ªå€™é€‰ï¼Œæ¨¡åž‹ç”ŸæˆæŽ¥ä¸‹æ¥ä¸€ä¸ªè¯æ±‡çš„æ¦‚çŽ‡åˆ†å¸ƒã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªå€™é€‰è¯æ±‡ç»„åˆè®¡ç®—ä¸€ä¸ªè”åˆæ¦‚çŽ‡ï¼Œå¹¶é€‰æ‹©æ¦‚çŽ‡æœ€é«˜çš„ä¸‰ä¸ªç»„åˆä½œä¸ºæ–°çš„å€™é€‰é›†ã€‚ä¾‹å¦‚ï¼Œå¯èƒ½çš„æ–°å€™é€‰é›†æ˜¯ï¼š[\"æˆ‘ çˆ±\"ï¼Œ\"ä½  æ˜¯\"ï¼Œ\"ä»– çš„\"]ã€‚\n",
      "3. é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æŸä¸ªå€™é€‰åºåˆ—ä»¥å¥å­ç»“æŸç¬¦ï¼ˆå¦‚å¥å·ï¼‰ç»“æŸï¼Œæˆ–è€…è¾¾åˆ°åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚\n",
      "`Beam search` å…è®¸ç®—æ³•åœ¨è§£ç è¿‡ç¨‹ä¸­è€ƒè™‘å¤šä¸ªå¯èƒ½çš„å€™é€‰ï¼Œä»Žè€Œ**åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¹³è¡¡äº†è´ªå¿ƒç®—æ³•ï¼ˆæ¯æ¬¡é€‰æ‹©æœ€ä¼˜å€™é€‰ï¼‰å’Œç©·ä¸¾æœç´¢ï¼ˆè€ƒè™‘æ‰€æœ‰å¯èƒ½çš„å€™é€‰ï¼‰ä¹‹é—´çš„æƒè¡¡**ã€‚é€šè¿‡è°ƒæ•´ beam sizeï¼Œæˆ‘ä»¬å¯ä»¥æŽ§åˆ¶ç®—æ³•åœ¨è§£ç è¿‡ç¨‹ä¸­æŽ¢ç´¢çš„å¹¿åº¦å’Œæ·±åº¦ã€‚è¾ƒå¤§çš„ beam size å¯ä»¥æé«˜æ‰¾åˆ°æœ€ä¼˜è§£çš„å¯èƒ½æ€§ï¼Œä½†åŒæ—¶ä¹Ÿä¼šå¢žåŠ è®¡ç®—æˆæœ¬ã€‚\n",
      "éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒBeam search å¹¶ä¸ä¿è¯èƒ½æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¼šåœ¨è§£ç è¿‡ç¨‹ä¸­ä¸¢å¼ƒæœ€ç»ˆçš„æœ€ä¼˜è·¯å¾„ã€‚æ­¤å¤–ï¼ŒBeam search ä¹Ÿä¸ä¿è¯èƒ½å¾—åˆ°å¤šæ ·æ€§å¾ˆå¼ºçš„è¾“å‡ºï¼Œå› ä¸ºå®ƒå€¾å‘äºŽé€‰æ‹©æ¦‚çŽ‡è¾ƒé«˜çš„å€™é€‰ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„å¥å­æ¯”è¾ƒå•ä¸€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ‰æ—¶ä¼šé‡‡ç”¨ä¸€äº›é¢å¤–çš„æŠ€æœ¯ï¼Œå¦‚éšæœºåŒ–æˆ–é•¿åº¦æƒ©ç½šï¼Œæ¥å¢žåŠ è¾“å‡ºå¥å­çš„å¤šæ ·æ€§ã€‚ docs[0] lens: 1007\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500, separator='\\n')\n",
    "docs = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "for i,d in enumerate(data):\n",
    "    splits = text_splitter.split_text(d)\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in splits]  # ç”Ÿæˆå”¯ä¸€çš„ ID\n",
    "    ids.extend(doc_ids)\n",
    "    docs.extend(splits)\n",
    "    metadatas.extend([{\"source\": str(sources[i])}] * len(splits))\n",
    "print(docs[0], 'docs[0] lens:', len(docs[0]))\n",
    "# print(\"-\"*100)\n",
    "# print(metadatas[0])\n",
    "# print(\"-\"*100)\n",
    "# print(ids[0])\n",
    "# print(type(metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143a04d-e63e-4585-946a-9607732bb8a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ä½¿ç”¨ faiss æŠŠæ–‡æœ¬è½¬æ¢ä¸º vector, é€‰æ‹©huggingfaceå¼€æºçš„æ¨¡åž‹ text2vecï¼›é€šè¿‡faissæž„å»ºå­˜å‚¨åˆ°ç£ç›˜çš„çŸ¥è¯†å‘é‡åº“\n",
    "ä»£ç å¦‚ä¸‹ï¼šï¼ˆ**å› ä¸ºæ— æ³•å®‰è£…faisså¯¼è‡´æ— æ³•ä½¿ç”¨**ï¼‰Â Â Â Â Â Â Â \n",
    "```pythonÂ Â Â Â Â Â Â Â Â Â Â \n",
    "store = FAISS.from_texts(docs, HuggingFaceEmbeddings(model_name=\"shibing624/text2vec-base-multilingual\"), metadatas=metadatas)\n",
    "faiss.write_index(store.index, \"docs.index\")Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
    "store.index = None\n",
    "with open(\"faiss_store.pkl\", 'wb') as f:\n",
    "    pickle.dump(store, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e7205-552e-4247-9802-4f0ac2ac75d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# å‘çŽ° milvusä¸å¥½ç”¨ï¼Œæ“ä½œå¤ªå¤æ‚äº†ï¼Œä½¿ç”¨ chromeadb å¹³æ›¿\n",
    "```python\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Š(å¦‚æžœæŠ¥é”™æˆ–è€…é‚£é‡Œæœ‰æœ‰é—®é¢˜ï¼Œå¯¼è‡´æŸ¥ä¸å‡ºåŽŸå› å°±åˆ æŽ‰è¯•è¯•)Â Â Â \n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯\n",
    "client = chromadb.Client()\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªé›†åˆç”¨äºŽå­˜å‚¨æ–‡æ¡£åŠå…¶åµŒå…¥\n",
    "try:\n",
    "    client.delete_collection(name=\"my_collection\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting collection: {e}\")\n",
    "\n",
    "collection = client.create_collection(name=\"my_collection\")\n",
    "\n",
    "# å®šä¹‰åµŒå…¥å‡½æ•°\n",
    "model_name = r\"D:\\CodeLibrary\\ChatGLM\\text2vecmul\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embeddings = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# def embed_texts(texts):\n",
    "#     inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "#     return embeddings.tolist()\n",
    "\n",
    "# èŽ·å–åµŒå…¥å‘é‡Â \n",
    "\n",
    "# docs = [\"document1\", \"document2\", \"document3\"]Â Â Â Â Â Â \n",
    "# metadatas = [{\"meta\": \"data1\"}, {\"meta\": \"data2\"}, {\"meta\": \"data3\"}]\n",
    "# embeddings = embed_texts(docs)\n",
    "\n",
    "# å‘é›†åˆä¸­æ·»åŠ æ•°æ®\n",
    "collection.add(ids=ids, documents=docs, metadatas=metadatas, embeddings=embeddings)\n",
    "\n",
    "\"\"\"\n",
    "    (å› ä¸ºæ— æ³•é€šè¿‡pickleåºåˆ—åŒ– clientä¸­çš„æŸäº›é“¾æŽ¥, æ•…ç”¨dictæ–¹å¼è¿›è¡Œæ•°æ®åºåˆ—åŒ–ï¼Œä½†æ˜¯æ²¡ç”¨ï¼Œå› ä¸ºåŽç»­å…¶å®žéœ€è¦æ•´ä¸ªdbçš„å†…å®¹åºåˆ—åŒ–åŽçš„åŠ è½½)\n",
    "\"\"\"\n",
    "# æå–é›†åˆä¸­çš„æ•°æ®\n",
    "collection_data = {Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
    "    \"ids\": docs,\n",
    "    \"documents\": docs,\n",
    "    \"metadatas\": metadatas,\n",
    "    \"embeddings\": embeddings,\n",
    "}\n",
    "\n",
    "# å°†é›†åˆæ•°æ®ä¿å­˜åˆ°æ–‡ä»¶\n",
    "with open(\"chroma_collection_data.pkl\", 'wb') as f:\n",
    "    pickle.dump(collection_data, f)\n",
    "# å…³é—­å®¢æˆ·ç«¯ï¼ˆchromadb æ²¡æœ‰ closeè¿™ç§æ“ä½œï¼‰Â Â Â \n",
    "# client.close()Â Â Â Â Â Â Â Â \n",
    "\n",
    "print(\"done!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a37364-ba90-4aa3-9444-f9ff315a8ded",
   "metadata": {},
   "source": [
    "# ä¸Šé¢çš„ä»£ç åˆå› ä¸ºæ˜¯ç›´æŽ¥ç”¨ chromadbï¼Œåœ¨åºåˆ—åŒ–ç´¢å¼•è¿™ä¸€å—åˆæœ‰é—®é¢˜ï¼Œä¸çŸ¥é“æ€Žä¹ˆè§£å†³Â Â Â \n",
    "\n",
    "# æ•…ï¼Œæ ¹æ® langchainæ–‡æ¡£ï¼Œæ”¹ç”¨ langchainä¸­ é›†æˆçš„ chroma\n",
    "- å‰æä»ç„¶æ˜¯è¦å®‰è£… `chromadb`\n",
    "  ```python\n",
    "  pip install chromadb\n",
    "  ```\n",
    "- **ä¸å†ä½¿ç”¨** `client = chromadb.Client()` åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œç„¶åŽå†åˆ›å»º `collection`\n",
    "- **æ”¹æˆäº†** é€šè¿‡å¯¼åŒ… `from langchain.vectorstores import Chroma`, ç„¶åŽé€šè¿‡ `db = Chroma.from_documentsï¼ˆdoc, embeddingsï¼‰` æ¥èŽ·å–\n",
    "\n",
    "# æ›´æ–°åŽçš„ä»£ç å¦‚ä¸‹:Â Â Â Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36b9e6-5f29-4694-b303-02377fe97e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# embedding_model å’Œ tokenizerèŽ·å–æ–¹å¼ä¸å˜\n",
    "embed_model_name = r\"D:\\CodeLibrary\\ChatGLM\\text2vecmul\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "\n",
    "def embed_texts(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = embed_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    return embeddings.tolist()\n",
    "\n",
    "# embeddings = embed_texts(docs)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)\n",
    "\n",
    "# å°†æ–‡æœ¬å’Œå…ƒæ•°æ®å°è£…æˆDocumentå¯¹è±¡Â Â \n",
    "documents = [Document(page_content=text, metadata=meta) for text, meta in zip(docs, metadatas)]\n",
    "print(type(metadatas))\n",
    "store = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "store.persist(\"chroma_store\")  # æŒä¹…åŒ–å­˜å‚¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0e9d0-d663-48b5-9076-e75aea810b6e",
   "metadata": {},
   "source": [
    "# åˆ©ç”¨ `inspect` æŸ¥çœ‹æºç Â Â Â Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe30d6c-3c9c-4bd9-8824-df06eebe2009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class RetrievalQA(BaseRetrievalQA):\n",
      "    \"\"\"Chain for question-answering against an index.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.llms import OpenAI\n",
      "            from langchain.chains import RetrievalQA\n",
      "            from langchain.faiss import FAISS\n",
      "            from langchain.vectorstores.base import VectorStoreRetriever\n",
      "            retriever = VectorStoreRetriever(vectorstore=FAISS(...))\n",
      "            retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    retriever: BaseRetriever = Field(exclude=True)\n",
      "\n",
      "    def _get_docs(\n",
      "        self,\n",
      "        question: str,\n",
      "        *,\n",
      "        run_manager: CallbackManagerForChainRun,\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Get docs.\"\"\"\n",
      "        return self.retriever.get_relevant_documents(\n",
      "            question, callbacks=run_manager.get_child()\n",
      "        )\n",
      "\n",
      "    async def _aget_docs(\n",
      "        self,\n",
      "        question: str,\n",
      "        *,\n",
      "        run_manager: AsyncCallbackManagerForChainRun,\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Get docs.\"\"\"\n",
      "        return await self.retriever.aget_relevant_documents(\n",
      "            question, callbacks=run_manager.get_child()\n",
      "        )\n",
      "\n",
      "    @property\n",
      "    def _chain_type(self) -> str:\n",
      "        \"\"\"Return the chain type.\"\"\"\n",
      "        return \"retrieval_qa\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import chromadb\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# èŽ·å–æºç \n",
    "source_code = inspect.getsource(RetrievalQA)\n",
    "\n",
    "print(source_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac60cd6-9bb5-4228-b701-1738e3fd9eb1",
   "metadata": {},
   "source": [
    "# æŽ¥ä¸‹æ¥æ˜¯ è®¿é—®çŸ¥è¯†åº“ä¸Žç”¨æˆ·äº¤äº’Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
    "\n",
    "è¿™é‡Œä½¿ç”¨ langchain API `RetrievalQA` æ¥è®¿é—®çŸ¥è¯†åº“Â Â Â Â Â Â Â Â \n",
    "\n",
    "1. ä»Žç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“Â Â Â \n",
    "```pythonÂ Â Â \n",
    "index = faiss.read_index('./docs.index') # ä¹‹å‰ç”Ÿæˆçš„æ•°æ®åº“index\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./faiss_store.pkl\", 'rb') as f:Â Â Â \n",
    "    store = pickle.load(f)Â Â Â \n",
    "\n",
    "store.index = indexÂ Â Â \n",
    "```\n",
    "\n",
    "2. è®¾è®¡ä¸€ä¸ªprompt æ¨¡æ¿Â Â Â \n",
    "```pythonÂ Â Â \n",
    "template = \"\"\"åŸºäºŽä»¥ä¸‹ä¿¡æ¯æ¥å›žç­”ç”¨æˆ·é—®é¢˜ã€‚Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
    "å·²çŸ¥ä¿¡æ¯ï¼šÂ Â Â \n",
    "{context}Â Â Â \n",
    "é—®é¢˜ï¼š\n",
    "{question}\"\"\"Â Â Â Â \n",
    "```\n",
    "\n",
    "3. ä½¿ç”¨ `RetrievalQA` æ¥ åŸºäºŽå¯¹çŸ¥è¯†åº“çš„è®¿é—®ç»“æžœ ä¸Ž LLM è¿›è¡Œäº¤äº’Â Â Â Â \n",
    "```pythonÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
    "prompt = PromplateTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}Â Â Â Â \n",
    "qa = RetrievalQA.from_chain_type(llm=chatglm, retriever=store.as_retriever(), chain_type=\"stuff\",\n",
    "                                chain_type_kwargs=chain_type_kwargs, return_source_documents=True)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "4. æž„å»ºä¸€ä¸ªæŸ¥è¯¢è¾“å…¥æ¡†ï¼Œä»ŽLLMè¿”å›žç»“æžœä¸­ï¼Œæå–é—®é¢˜ç­”æ¡ˆå’ŒçŸ¥è¯†åº“æ–‡æ¡£å¼•ç”¨æ¥æºï¼šÂ Â Â \n",
    "```pythonÂ Â Â \n",
    "query = input(\"\\nè¯·è¾“å…¥é—®é¢˜ï¼š\")Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
    "\n",
    "res = qa(query)Â Â Â \n",
    "answer, doc = res['result'], res['source_documents']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n> é—®é¢˜ï¼š')Â Â Â Â Â \n",
    "print(query)Â Â Â Â \n",
    "print(\"\\n> å›žç­”ï¼š\")Â Â Â Â Â Â Â Â Â \n",
    "print(answer)Â Â Â Â Â \n",
    "print(\"\\n> æ¥æºï¼š\")Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â \n",
    "for document in docs:Â Â \n",
    "    print(\"\\n> \" + str(document.metadata['source']) + \":\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ca067-29d7-4a63-ad69-514f7997f2f6",
   "metadata": {},
   "source": [
    "# chroma ç‰ˆæœ¬çš„ çŸ¥è¯†åº“è®¿é—® ä¸Ž ç”¨æˆ·äº¤äº’\n",
    "1. chroma ç´¢å¼•åŠ è½½\n",
    "2. prompt template å®šä¹‰Â Â \n",
    "3. ç”¨æˆ·äº¤äº’å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84038e2-8b67-461b-ae33-30a17bdf3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Chroma.load(\"chroma_store\", embeddings)\n",
    "\n",
    "template = \"\"\"åŸºäºŽä»¥ä¸‹ä¿¡æ¯æ¥å›žç­”ç”¨æˆ·é—®é¢˜ã€‚                          \n",
    "å·²çŸ¥ä¿¡æ¯ï¼š \n",
    "{context} \n",
    "é—®é¢˜ï¼š\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":prompt}\n",
    "qa = RetrievalQA.from_chain_type(llm=chatglm, retriever=store.as_retriever(), chain_type=\"stuff\",\n",
    "                                chain_type_kwargs=chain_type_kwargs, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890d5fe-5156-4355-8e55-77566884d79d",
   "metadata": {},
   "source": [
    "# è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123fc0d-c134-4f05-9770-9e04d62f2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"\\nè¯·è¾“å…¥é—®é¢˜ï¼š\")                       \n",
    "\n",
    "res = qa(query) \n",
    "answer, doc = res['result'], res['source_documents']\n",
    "\n",
    "print('\\n\\n> é—®é¢˜ï¼š')   \n",
    "print(query)  \n",
    "print(\"\\n> å›žç­”ï¼š\")       \n",
    "print(answer)   \n",
    "print(\"\\n> æ¥æºï¼š\")                      \n",
    "for document in docs:\n",
    "    print(\"\\n> \" + str(document.metadata['source']) + \":\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
