from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
import chainlit as cl
from openai import AsyncOpenAI

from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema.runnable import RunnablePassthrough
from langchain.callbacks.base import BaseCallbackHandler
from langchain.indexes import SQLRecordManager, index
from CustomEmbeddings import BGEEmbeddings

from langchain_community.tools.ddg_search import DuckDuckGoSearchRun
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain import hub
import os
from langchain.memory import ConversationBufferMemory
import json
from zhipuai import ZhipuAI
import base64

with open('config.json', 'r') as f:
    config = json.load(f)

os.environ["LANGCHAIN_API_KEY"] = config["LANGCHAIN_API_KEY"]

DATA_PATH = './data'
EMBED_MODEL_PATH = '/home/yhchen/huggingface_model/BAAI/bge-m3'
MODEL_BASE_URL = config["BASE_URL"] # http://localhost:8080/v1
MODEL_API_KEY = config["API_KEY"]   # token-qwen2
MODEL_NAME = config["MODEL_ID"]
EMBEDDING_MODEL_NAME = config["EMBEDDING_MODEL_NAME"]
EMBEDDING_BASE_ULR = config["EMBEDDING_BASE_ULR"]

"""
############################################################################################################
    ä¸€äº›ä¼šä½¿ç”¨åˆ°çš„ é€šç”¨å‡½æ•°
############################################################################################################
"""
def LLM_Client():
    client = AsyncOpenAI(
        api_key=MODEL_API_KEY, # vllm
        base_url=MODEL_BASE_URL,
    )
    settings = {
        "model": MODEL_NAME, 
        "max_tokens": 512,
        "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥å‡å°‘é‡å¤
        "top_p": 0.9,        # è°ƒæ•´ top_p ä»¥æ§åˆ¶è¾“å‡ºå¤šæ ·æ€§
    }
    return client, settings


def Process_Data_Create_Retriever(data_path: str, embed_model_path: str):
    """
        å¤„ç†æ•°æ®, å¹¶åˆ›å»º retriever(æ£€ç´¢å™¨) 
    """
    loader = DirectoryLoader(data_path, glob="*.txt")
    docs = loader.load()
    documents = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(docs)

    # embeddings = HuggingFaceBgeEmbeddings(model_name=embed_model_path)
    embeddings = BGEEmbeddings(base_url=EMBEDDING_BASE_ULR, model_name=EMBEDDING_MODEL_NAME)

    vectordb = Chroma.from_documents(documents=documents, embedding=embeddings, collection_name="mydata")
    retriever = vectordb.as_retriever()

    # å°† chromaçš„æ•°æ®åŠæ•°æ®å…³ç³»ä½œä¸ºç´¢å¼•å­˜å‚¨åˆ° SQLiteä¸­ (æé«˜ç´¢å¼•é€Ÿåº¦çš„å…³é”®)
    namespace = "chromadb/my_documents"
    record_manager = SQLRecordManager(
        namespace, db_url="sqlite:///record_manager_cache.sql"
    )
    record_manager.create_schema()

    index_result = index(
        docs,
        record_manager,
        vectordb,
        cleanup="incremental",
        source_id_key="source",
    )

    print(f"Indexing stats: {index_result}")

    return retriever


"""
############################################################################################################
    å¢åŠ å‡ ä¸ª æç¤ºå¯åŠ¨å™¨ (ç±»ä¼¼äºæ¨èprompt or chat)
############################################################################################################
"""
@cl.set_starters
async def set_starters():
    return [
        cl.Starter(
            label="æ—©æ™¨ä¾‹è¡Œç¨‹åºæ„æƒ³",
            message="ä½ èƒ½å¸®æˆ‘åˆ›å»ºä¸€ä¸ªä¸ªæ€§åŒ–çš„æ—©æ™¨ä¾‹è¡Œç¨‹åºå—ï¼Ÿè¿™èƒ½å¸®åŠ©æˆ‘åœ¨ä¸€å¤©ä¸­æé«˜ç”Ÿäº§åŠ›ã€‚å…ˆé—®æˆ‘å…³äºæˆ‘ç°åœ¨çš„ä¹ æƒ¯ä»¥åŠå“ªäº›æ´»åŠ¨èƒ½åœ¨æ—©ä¸Šç»™æˆ‘å¸¦æ¥æ´»åŠ›ã€‚",
            icon="/public/idea.svg",
            ),

        cl.Starter(
            label="åƒäº”å²å°å­©ä¸€æ ·è§£é‡Šè¶…å¯¼ä½“",
            message="åƒå¯¹äº”å²å°å­©è§£é‡Šä¸€æ ·ç®€å•åœ°è¯´æ˜ä»€ä¹ˆæ˜¯è¶…å¯¼ä½“ã€‚",
            icon="/public/learn.svg",
            ),
        cl.Starter(
            label="ç”¨äºæ¯æ—¥é‚®ä»¶æŠ¥å‘Šçš„Pythonè„šæœ¬",
            message="ç¼–å†™ä¸€ä¸ªPythonè„šæœ¬æ¥è‡ªåŠ¨åŒ–å‘é€æ¯æ—¥é‚®ä»¶æŠ¥å‘Šï¼Œå¹¶æŒ‡å¯¼æˆ‘å¦‚ä½•è®¾ç½®ã€‚",
            icon="/public/terminal.svg",
            ),
        cl.Starter(
            label="é‚€è¯·æœ‹å‹å‚åŠ å©šç¤¼çš„çŸ­ä¿¡",
            message="å†™ä¸€æ¡çŸ­ä¿¡ï¼Œé‚€è¯·æœ‹å‹ä¸‹ä¸ªæœˆä½œä¸ºæˆ‘çš„ä¼´éƒå‚åŠ å©šç¤¼ã€‚æˆ‘å¸Œæœ›ä¿æŒéå¸¸ç®€çŸ­å’Œéšæ„çš„é£æ ¼ï¼Œå¹¶æä¾›ä¸€ä¸ªå©‰æ‹’çš„é€‰é¡¹ã€‚",
            icon="/public/write.svg",
            )
        ]


"""
############################################################################################################
        èŠå¤©é…ç½®æ–‡ä»¶
############################################################################################################
"""
@cl.set_chat_profiles
async def chat_profile():
    return [
        cl.ChatProfile(
            name="Qwen2-RAG",
            markdown_description="The underlying LLM model is **Qwen2-72B-Instruct-GPTQ-INT4**.",
            icon="/public/qwen.png",
        ),
        cl.ChatProfile(
            name="Qwen2",
            markdown_description="The underlying LLM model is **Qwen2-72B-Instruct-GPTQ-INT4**.",
            icon="/public/qwen.png",
        ),
        cl.ChatProfile(
            name="Qwen2-LC",
            markdown_description="The underlying LLM model is **Qwen2-72B-Instruct-GPTQ-INT4w**.",
            icon="/public/qwen.png",
        ),
        cl.ChatProfile(
            name="Agent",
            markdown_description="The underlying LLM model is **GLM4**.",
            icon="/public/google.png",
        ),
        cl.ChatProfile(
            name="InternVL2",
            markdown_description="The underlying LLM model is **InternVL2-8B**.",
            icon="/public/google.png",
        ),
        cl.ChatProfile(
            name="ImageGen",
            markdown_description="The underlying LLM model is **GLM4**.",
            icon="/public/google.png",
        ),
        cl.ChatProfile(
            name="GraphRAG-latest-global",
            markdown_description="The underlying LLM model is **Phi-3**.",
            icon="/public/microsoft.png",
        ),
        cl.ChatProfile(
            name="GraphRAG-latest-local",
            markdown_description="The underlying LLM model is **Phi-3**.",
            icon="/public/microsoft.png",
        ),
        cl.ChatProfile(
            name="Llama3",
            markdown_description="The underlying LLM model is **Llama3**.",
            icon="/public/meta.png",
        ),
    ]


"""
############################################################################################################
    @cl.on_chat_start ä¸­ ä¸åŒåŠŸèƒ½æ¨¡å‹å°è£… (æ–°å¢æ¨¡å‹éœ€è¦æ·»åŠ )
############################################################################################################
"""
######################################################################################
def LC_Chat_Model():
    """
        model chat of RAG (based langchain)
    """
    model = ChatOpenAI(
            base_url= MODEL_BASE_URL,
            api_key = MODEL_API_KEY,
            model = MODEL_NAME,
            streaming=True,
            )
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯ä¸€ä¸ªèªæ˜çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”ä»»ä½•äº‹æƒ…",
            ),
            ("human", "{question}"),
        ]
    )
    runnable = prompt | model | StrOutputParser()

    return runnable


######################################################################################
def RAG_Chat_Model(data_path, embed_mode_path):
    """
        RAG model chat
    """
    model = ChatOpenAI(
            base_url= MODEL_BASE_URL,
            api_key = MODEL_API_KEY,
            model = MODEL_NAME,
            streaming=True,
            )
    
    system_prompt = (
        "æ‚¨æ˜¯ä¸€ä¸ªç”¨äºé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚"
        "ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”é—®é¢˜ã€‚"
        "å¦‚æœæ‚¨ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯´æ‚¨ä¸çŸ¥é“ï¼Œä¸è¦çŒœæµ‹å›ç­”ã€‚"
        "æœ€å¤šä½¿ç”¨äº”å¥è¯ï¼Œå°½é‡ä¿æŒå›ç­”ç®€æ´ã€‚"
        "\n\n"
        "{context}"
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{question}")
        ]
    )

    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    retriever = Process_Data_Create_Retriever(data_path, embed_mode_path)

    runnable = (
        {"context": retriever | format_docs, "question":RunnablePassthrough()} | prompt | model | StrOutputParser()
        )
    return runnable


######################################################################################
def GraphRAG_Local_Model():
    """
        local search of GraphRAG
    """
    # é‡Œé¢çš„å‚æ•°ä¸èƒ½ä¹±åŠ  
    model = AsyncOpenAI(
        base_url="http://localhost:20213/v1",
        api_key=MODEL_API_KEY,
    )
    settings = {
        "model": "GraphRAG-latest-local", 
        "max_tokens": 512,
        "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥å‡å°‘é‡å¤
        "top_p": 0.9,        # è°ƒæ•´ top_p ä»¥æ§åˆ¶è¾“å‡ºå¤šæ ·æ€§
    }
    return model, settings


######################################################################################
def GraphRAG_Global_Model():
    """
        global search of GraphRAG
    """
    model = AsyncOpenAI(
        base_url="http://localhost:20213/v1",
        api_key=MODEL_API_KEY,
    )
    settings = {
        "model": "GraphRAG-latest-global", 
        "max_tokens": 512,
        "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥å‡å°‘é‡å¤
        "top_p": 0.9,        # è°ƒæ•´ top_p ä»¥æ§åˆ¶è¾“å‡ºå¤šæ ·æ€§
    }
    return model, settings


######################################################################################
def Agent_Chat_Model():
    """
        agent chat
        tools: search, retriever
    """

    model = ChatOpenAI(
            base_url= MODEL_BASE_URL,
            api_key = MODEL_API_KEY,
            model = MODEL_NAME,
            streaming=True,
            )

    # tools prepare
    search = DuckDuckGoSearchRun(max_results=2)
    # retriever = Process_Data_Create_Retriever(data_path, embed_mode_path)

    # create retriever_tools
    # retriever_tool = create_retriever_tool(
    #     retriever, 
    #     name="documents search",
    #     description="search for info about documents data"
    # )

    # tools = [search, retriever_tool]
    tools = [search]

    # prompt = hub.pull("hwchase17/structured-chat-agent")  # ç»“æ„åŒ–çš„promptï¼Œæœ‰agentè¿è¡Œè¿‡ç¨‹ (ç”¨create_structured_chat_agent)
    prompt = hub.pull("hwchase17/openai-functions-agent")   # ç›´æ¥è¿”å›ç»“æœ

    # æ·»åŠ è®°å¿† ï¼ˆå³ å†å²å¯¹è¯ä¿¡æ¯ï¼‰
    history = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")

    # create agent
    # agent = create_structured_chat_agent(llm=model, tools=tools, prompt=prompt)
    # agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=3)

    agent = create_tool_calling_agent(llm=model, tools=tools, prompt=prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, memory=history)

    return agent_executor


######################################################################################
# def Image_Gen_Model():
#     """
#         image gen
#     """
#     pass


######################################################################################
# def Multi_Chat_Model():
#     """
#         multi-modal model, image-text
#     """
#     model = AsyncOpenAI(
#         base_url="http://0.0.0.0:8081/v1",
#         api_key="token-internvl2",
#     )
#     settings = {
#         "model": "internvl2", 
#         "max_tokens": 512,
#         "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥å‡å°‘é‡å¤
#         "top_p": 0.9,        # è°ƒæ•´ top_p ä»¥æ§åˆ¶è¾“å‡ºå¤šæ ·æ€§
#     }
#     return model, settings



"""
############################################################################################################
    
                                ==================
                                = Chainlit è£…é¥°å™¨ =   
                                ==================

    èŠå¤©æ¡† åŠŸèƒ½, æ¨¡æ¿, langchain é›†æˆ ç­‰ (æ–°å¢æ¨¡å‹éœ€è¦æ·»åŠ )

############################################################################################################
"""
@cl.on_chat_start
async def on_chat_start():
    # ä»é€‰æ‹©çš„æ¨¡å‹åˆ¤æ–­ ä½¿ç”¨åŠŸèƒ½
    model_name = cl.user_session.get("chat_profile")

    if(model_name == 'Qwen2-LC'):
        runnable = LC_Chat_Model()
        cl.user_session.set("runnable", runnable)
    
    elif (model_name == 'Qwen2'):
        cl.user_session.set(
            "message_history",
            [{"role": "system", "content": "You are a helpful assistant."}],
        )

    elif (model_name == "Qwen2-RAG"):
        runnable = RAG_Chat_Model(DATA_PATH, EMBED_MODEL_PATH)
        cl.user_session.set("runnable", runnable)

    elif (model_name == "Agent"):
        agent_executor = Agent_Chat_Model()
        cl.user_session.set("agent_executor", agent_executor)

    elif (model_name == "GraphRAG-latest-global"):
        cl.user_session.set(
            "message_history",
            [{"role": "system", "content": "You are a helpful assistant."}],
        )

    elif (model_name == "GraphRAG-latest-local"):
        cl.user_session.set(
            "message_history",
            [{"role": "system", "content": "You are a helpful assistant."}],
        )

    elif (model_name == "ImageGen"):
        client = ZhipuAI(api_key=config["API_KEY"])
        cl.user_session.set("client", client)

    # elif (model_name == "InternVL2"):
    #     client, settings = Multi_Chat_Model()
    #     cl.user_session.set("client", client)
    #     cl.user_session.set("settings", settings)


"""
############################################################################################################
    @cl.on_message ä¸­ ä¸åŒåŠŸèƒ½æ¨¡å‹å°è£…  (æ–°å¢æ¨¡å‹éœ€è¦æ·»åŠ )
############################################################################################################
"""
######################################################################################
async def LC_Chat_Message(message):
    """
        ä½¿ç”¨ langchain å®ç°çš„èŠå¤©å¯¹è¯
    """
    runnable = cl.user_session.get("runnable")  # type: Runnable

    msg = cl.Message(content="")

    async for chunk in runnable.astream(
        {"question": message.content},
        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),
    ):
        await msg.stream_token(chunk)

    await msg.send()


######################################################################################
async def Chat_Model_Message(message):
    """
        original model chat message
    """
    # è·å–èŠå¤©é…ç½®ä¸­æŒ‡å®šçš„æ¨¡å‹
    # chat_profile = cl.user_session.get("chat_profile")

    message_history = cl.user_session.get("message_history")
    message_history.append({"role": "user", "content": message.content})

    msg = cl.Message(content="")
    await msg.send()

    client, settings = LLM_Client()

    stream = await client.chat.completions.create(
        messages=message_history, stream=True, **settings
    )

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await msg.stream_token(token)

    message_history.append({"role": "assistant", "content": msg.content})
    await msg.update()


######################################################################################
async def RAG_Chat_Message(message):
    """
        RAG chat message 
        
        message: æ˜¯ä» @cl.on_message è£…é¥°å™¨æ¥æ”¶åˆ°çš„å‚æ•°ï¼Œè¡¨ç¤ºç”¨æˆ·å‘é€çš„æ¶ˆæ¯
        msg: msg ç”¨äºå­˜å‚¨å’Œå‘é€æ¨¡å‹ç”Ÿæˆçš„å›å¤ã€‚
    """
    runnable = cl.user_session.get("runnable")  # type: Runnable

    msg = cl.Message(content="")    # initial

    class PostMessageHandler(BaseCallbackHandler):
        """
        (è¿™ä¸ªç±»ä¸æ˜¯éå¿…éœ€çš„, åªæ˜¯æ ‡è®°èƒ½å¤Ÿæ‰¾åˆ°åŸæ–‡æ¡£çš„æ¥æº)
        Callback handler for handling the retriever and LLM processes.
        Used to post the sources of the retrieved documents as a Chainlit element.
        """

        def __init__(self, msg: cl.Message):
            BaseCallbackHandler.__init__(self)
            self.msg = msg
            self.sources = set()  # To store unique pairs

        def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):
            for d in documents:
                source_page_pair = (d.metadata['source'], d.metadata['page'])
                self.sources.add(source_page_pair)  # Add unique pairs to the set

        def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):
            if len(self.sources):
                sources_text = "\n".join([f"{source}#page={page}" for source, page in self.sources])
                self.msg.elements.append(
                    cl.Text(name="Sources", content=sources_text, display="inline")
                )

    async for chunk in runnable.astream(
        message.content,
        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler(), PostMessageHandler(msg)]),
    ):
        await msg.stream_token(chunk)

    await msg.send()


######################################################################################
async def Agent_Chat_Message(message):
    """
        Agent chat message
    """
    agent_executor = cl.user_session.get("agent_executor")  # type: Runnable

    if agent_executor is None:
        await cl.Message(content="Agent executor is not initialized. Please try again later.").send()

    msg = cl.Message(content="")

    input_data = {"input": message.content}
    result = agent_executor.invoke(input_data)['output']
    
    # æµå¼è¾“å‡º
    for char in result:
        await msg.stream_token(char)

    await msg.send()
    

######################################################################################
async def GraphRAG_Global_Model_Message(message):
    """
        graphrag global model chat message
    """
    # è·å–èŠå¤©é…ç½®ä¸­æŒ‡å®šçš„æ¨¡å‹
    # chat_profile = cl.user_session.get("chat_profile")

    message_history = cl.user_session.get("message_history")
    message_history.append({"role": "user", "content": message.content})

    msg = cl.Message(content="")
    await msg.send()

    model, settings = GraphRAG_Global_Model()

    stream = await model.chat.completions.create(
        messages=message_history, stream=True, **settings
    )

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await msg.stream_token(token)

    message_history.append({"role": "assistant", "content": msg.content})
    await msg.update()


######################################################################################
async def GraphRAG_Local_Model_Message(message):
    """
        graphrag local model chat message
    """

    message_history = cl.user_session.get("message_history")
    message_history.append({"role": "user", "content": message.content})

    msg = cl.Message(content="")
    await msg.send()

    model, settings = GraphRAG_Local_Model()

    stream = await model.chat.completions.create(
        messages=message_history, stream=True, **settings
    )

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await msg.stream_token(token)

    message_history.append({"role": "assistant", "content": msg.content})
    await msg.update()


######################################################################################
async def Image_Gen_Message(message):
    """
        image gen model chat message
    """
    model = cl.user_session.get("client")

    response = model.images.generations(
        model="cogview-3", #å¡«å†™éœ€è¦è°ƒç”¨çš„æ¨¡å‹ç¼–ç 
        prompt=message.content,
    )

    result_url = response.data[0].url

    image = cl.Image(url=result_url, name="image1", display="inline")

    await cl.Message(
        content="",
        elements=[image],
    ).send()



######################################################################################
async def Multi_Chat_Message(msg: cl.Message):
    """
        multi chat model chat message
    """
    if not msg.elements:
        await cl.Message(content="ğŸ¤— è¯·æä¾›å›¾ç‰‡è¿›è¡Œæé—®!").send()
        return

    # Processing images exclusively
    images = [file for file in msg.elements if "image" in file.mime]

    # Read the first image and convert to base64
    with open(images[0].path, "rb") as f:
        image_data = f.read()
    encoded_string = base64.b64encode(image_data).decode("utf-8")
    image_url = f"data:image/png;base64,{encoded_string}"

    # Use OpenAI API
    client = AsyncOpenAI(api_key='token-internvl2', base_url='http://localhost:8081/v1')
    model_name = "internvl2"
    response = await client.chat.completions.create(
        model=model_name,
        messages=[{
            'role': 'user',
            'content': [{
                'type': 'text',
                # 'text': 'describe this image',
                'text': msg.content,
            }, {
                'type': 'image_url',
                'image_url': {
                    'url': image_url
                },
            }],
        }],
        temperature=0.1,
        top_p=0.9,
        stream=True,
    )

    msg = cl.Message(content="")
    await msg.send()
    
    async for part in response:
        if token := part.choices[0].delta.content or "":
            await msg.stream_token(token)

    await msg.update()


"""
############################################################################################################
    
                                ==================
                                = Chainlit è£…é¥°å™¨ =   
                                ==================  

    å“åº”çš„æ¶ˆæ¯ä¼ é€’åˆ° WebUI ä¸Š (æ–°å¢æ¨¡å‹éœ€è¦æ·»åŠ )

############################################################################################################
"""
@cl.on_message
async def on_message(message: cl.Message):

    model_name = cl.user_session.get("chat_profile")

    if (model_name == 'Qwen2-LC'):
        await LC_Chat_Message(message)

    elif (model_name == "Qwen2"):
        await Chat_Model_Message(message)

    elif (model_name == "Qwen2-RAG"):
        await RAG_Chat_Message(message)

    elif (model_name == "Agent"):
        await Agent_Chat_Message(message)
    
    elif (model_name == "GraphRAG-latest-global"):
        await GraphRAG_Global_Model_Message(message)
    
    elif (model_name == "GraphRAG-latest-local"):
        await GraphRAG_Global_Model_Message(message)
    
    elif (model_name == "ImageGen"):
        await Image_Gen_Message(message)
    
    elif (model_name == "InternVL2"):
        await Multi_Chat_Message(message)

        